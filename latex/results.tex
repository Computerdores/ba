\subsection{Results}
It can generally be noted that the jitter on the TX side does not have an impact on any of the results
(See \cref{fig:results-bursty-65k-16k,fig:results-bursty-65k-2k,fig:results-basic-65k}).
Jitter having a significant impact can be a problem where the precise timing by the benchmark causes significant changes in performance.
This was also observed at times during the development of the benchmarks, but disappeared after applying bug fixes.

Furthermore, regarding comparisons to the EQueue paper, three things are of note.
First, the measurements from the EQueue paper include parts of their measurement code (cite source code here), my measurements do not.
However, it is not clear to me how significant the impact on their measurements would be.
Second, I suspect that the measurements with different burst sizes from the EQueue paper were done in their cross-cpu configuration,
which would be a significant difference to my measurements, but the paper does not specify whether or not this is the case.
I suspect this, because the values for burst size 2048 from the graph match the cross-cpu values from their table better than the same-cpu values.
Third, the values for the different burst sizes are not directly stated and were therefore discerned from the graph included in the paper.

In the following subsections, I analyse the results from the different benchmarks.
I first address the two bursty benchmarks and compare the results to the results from the EQueue paper, as they also used bursty enqueue timing.
Lastly, for the Basic benchmark I compare my results to those of the B-Queue and FastForward papers, as this benchmark is, out of mine, most similar to theirs.

\input{figures/plots/eq16k-bursty}
\input{figures/plots/eq2k-bursty}
\input{figures/plots/eq-basic}

\todo{TODO: My numbers here are read from figures, replace them with the accurate values}

\subsubsection{Bursty 65k-16k}
\begin{itemize}
    \item see results in \autoref{fig:results-bursty-65k-16k}
    \item no jitter and measuring failed ops is closest of the benchmarks to the benchmarking in EQueue paper
        with 16k burst size (see \autoref{fig:results-bursty-65k-16k-b})
    \item at 16k burst size EQueue reportedly took $\approx 50\text{ cycles} \approx \SI{30}{\nano\second}$
        (see equeue paper)
    \item My measurements show $\approx \SI{26}{\nano\second}$ (RX) and $\approx \SI{24}{\nano\second}$ (TX),
        less than \SI{5}{\nano\second} off
    \item gives credence to idea that this benchmark is similar to that of EQ paper
    \item also suggests, that my version of EQ impl is very similar to their version
    \item Other queues tested in EQueue paper (BQ, MCRB, FFWDQ) reportedly took $\approx \SI{108}{\nano\second}$
    \item My measurements show $\approx \SI{23}{\nano\second}$ (mean of RX and TX)
    \item other queues 4-5x worse in EQueue paper
    \item the reasons for this difference are unclear; could be due to different queue parameters for BQ, MCRB, FFWDQ than I am using (EQueue paper is light on details about this)
    \item could also mean that EQ is only better than those queues in specific scenarios / hardware; more testing needed
    \item so while the EQueue performance lines up between the two, other queues perform much better for me
    \item we can also see that, without measuring failed operations, RX of all but LPRT perform 50\% better
    \item meanwhile TX performance only improves marginally
    \item suggests that a significant number of RX operations fail due to empty queues in this benchmark
    \item suspect this is happening during wait time between bursts; thus lower time for LPRT doesn't matter
    \item => all queues perform roughly equally well here
\end{itemize}

\subsubsection{Bursty 65k-2k}
\begin{itemize}
    \item see results in \autoref{fig:results-bursty-65k-2k}
    \item no jitter and measuring failed ops is closest of the benchmarks to the benchmarking in EQueue paper
        with 2k burst size (see \autoref{fig:results-bursty-65k-2k-b})
    \item at 2k burst size EQ, BQ, MCRB, FFWDQ reportedly took $\approx 50\text{ cycles} \approx \SI{30}{\nano\second}$
    \item in my measurements EQ takes $\approx \SI{35}{\nano\second}$ for TX, other queues $\approx
        \SI{20}{\nano\second}$
    \item for RX all queues take around 12-\SI{17}{\nano\second}
    \item besides TX of EQueue which is an outlier here all queues generally perform similarly, like they also did
        in EQueue paper
    \item queues are generally faster here than in EQueue paper though
    \item as mentioned earlier, I suspect the EQueue numbers here were measured in their cross-cpu
        configuration, could explain the performance difference
    \item when not measuring failed operations, the performances of the queues become evenmore similar (See
        \autoref{fig:results-bursty-65k-2k-a})
    \item suggests main performance difference here comes from failed operations due to full/empty queues
    \item on its own number of failed ops for EQ could be interpreted as bursts not entirely fitting in the queue leading to some but not very many failed ops
    \item unlikely since it doesn't show up with larger burst size where it would be expected to be evenmore significant
    \item unclear why this increase in failed ops is present
    \item MCRB is also a bit slower on RX when measuring failed ops
    \item => all other queues perform equally well to Bursty 65k-16k
\end{itemize}

\subsubsection{Basic 65k}
\begin{itemize}
    \item see results in \autoref{fig:results-basic-65k}
    \item the performance is mostly the same as in Bursty 65k-16k
    \item TX of BQ, EQ, LPRT is worse by $\approx 1$ order of magnitude (See \autoref{fig:results-basic-65k-b})
    \item without measuring failed operations this effect can't be seen for BQ and EQ and is much less
        drastic for LPRT (See \autoref{fig:results-basic-65k-a})
    \item suggests that for BQ and EQ this is mostly due to failing enqueue because of full queues
    \item whereas for LPRT this is also the case but additionally the enqueue operation also seems to be slower in itself
    \item this might mean that EQ/BQ/LPRT fill up early in benchmark -- causing enqueues to fail later eventhough dequeue similarly fast to others
    \item Lamport's TX being worse could be for Cache Opt reasons, BQ/EQ/FFWDQ/MCRB all do cache opt but LPRT doesn't
    \item this case is also somewhat similar to the testing from the FastForward paper, where FFWDQ and LPRT are compared
    \item worth noting that there are significant differences: much larger queue size here, they sent packets in a circle I do nothing of the sort
    \item FastForward paper showed FFWDQ to be significantly faster than LPRT which my results confirm
    \item it is also the most similar to the benchmark from the B-Queue paper with 1 concurrent queue
    \item the B-Queue benchmark showed very similar performance of BQ, MCRB and FFWDQ
    \item when not measuring failed operations I see this too, however, when measuring failed ops BQ's TX is much slower
    \item however it is worth noting that the BQ paper sees very small values for the operation time of the tested queues (\ns{5} - \ns{9}; depending on queue and hardware)
    \item neither my measurements, nor those of the EQueue paper show values that small for any of those queues
    \item this leads me to believe that the absolute values are not comparable between the hardware used in the B-Queue paper and the hardware used here or in the EQueue paper
    \item also, BQ used much smaller queue sizes for their tests
    \item overall, suggests performance of FFWDQ/FFLWQ/MCRB is most stable when enqueue rate is stable
\end{itemize}
