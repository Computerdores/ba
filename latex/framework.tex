\section{Framework}
\subsection{Goals}
\begin{itemize}
    \item Goal: framework for benchmarking different SPSC FIFO queues in different benchmarks
    \item should allow new queues or benchmark variations with minimal effort
    \item Goal: avoid duplicating the entire benchmark code for every queue/variation in a part of the benchmark
    \item duplicating the code would allow the compiler to optimise better
    \item Goal: don't want to give up on that performance advantage (I would expect that to be present in real applications)
    \item EQueue: parts of queue impl in benchmark code
    \item Goal: full separation of benchmark code and queue implementation
\end{itemize}

\subsection{Architecture}
\begin{itemize}
    \item different benchmarks are still similar in certain ways
    \item producer/consumer threads
    \item enqueue/dequeue X items
    \item take measurements per operation
    \item wait between operations to simulate work
    \item What varies is: waiting behaviour, measuring code
    \item => Have one runner class, 'skeleton benchmark', that makes use of swappable components for the waiting behaviour and measuring
    \item this way core part of benchmark is not duplicated (Goal)
    \item can plug different components into runner class to have different benchmarks (Goal)
    \item each component is implemented as a class with a certain interface
    \item structure of producer / consumer threads:
        \begin{itemize}
            \item largely the same; differences because one enqueues one dequeues
            \item run through loop to enqueue X items
            \item for each item: queue operation gets called repeatedly until it is successfull (e.g. if enqueue fails it will eventually succeed once dequeue has run on other thread)
            \item waiter gets called once before the first operation of first item (allows it to adjust wait durations relative to start time) and after every successful operation
            \item to take before measurements, measurer either gets called only before the first attempt of each item (when including failed operations in the measurements)
                or before every attempt to reset the before measurement from failed operations (when not including failed operations in the measurements)
            \item measurer also gets called after every successful operation to take after measurements
        \end{itemize}
\end{itemize}

\subsection{Technical Details}

\input{figures/runner-class-code}

\begin{itemize}
    \item see runner class implementation: \autoref{fig:runner-class-code}
    \item components as template parameters
    \item template params are pairs of components
        \begin{itemize}
            \item not that important
            \item only done to shorten construction of runner class
        \end{itemize}
    \item components defined in headers
    \item => get inlined into the runner class (verified in Ghidra)
    \item => almost identical result of compilation to duplicating the runner class and copying the component
        code into it
    \item => no performance difference, but easier to work with (Goal)
    \item Queue impl is also template parameter => full separation of Queue impl and benchmark (Goal)
    \item waiter
        \begin{itemize}
            \item \texttt{start} method -- marks the beginning of the benchmark
            \item \texttt{wait} method -- triggers a wait time according to the waiter's implementation
        \end{itemize}
    \item measurer
        \begin{itemize}
            \item \texttt{pre} method -- gets called to record timestamp before operation (overwrites last
                timestamp if called back to back)
            \item \texttt{post} method -- gets called to record timestamp after operation
        \end{itemize}
    \item queue
        \begin{itemize}
            \item the data structure to be benchmarked
            \item implements consistent interface with non-blocking \texttt{enqueue} and \texttt{dequeue}
                that indicate success via their return values (See \autoref{fig:queue-interface})
        \end{itemize}
    \item main function prepares/runs/cleans up benchmark
        \begin{itemize}
            \item loads numerical parameters related to queues and benchmark from config file
            \item starts an instance of the runner class with the appropriate components
            \item causes write of result data after benchmark is done
        \end{itemize}
\end{itemize}

\subsubsection{Time Measurement}
\begin{itemize}
    \item relevant for waiters and measurers
    \item I am using \texttt{RDTSC} to measure time
    \item does not meaure time directly; measures clock ticks
    \item incremented at constant rate (CPU base frequency) if CPU has invariant TSC (The CPU I use does)
    \item so to convert \texttt{RDTSC} diff to seconds divide by base freq
\end{itemize}

\subsubsection{Waiter Implementations}
\begin{itemize}
    \item causes a delay between operation (\textit{waits})
    \item used to emulate different enqueue/dequeue patterns
    \item Constant Wait
        \begin{itemize}
            \item waits a constant amount of time between operations
            \item used to emulate operations (of input / output) that take a very consistent amount of time
        \end{itemize}
    \item Constant Rate
        \begin{itemize}
            \item tries to maintain a constant time difference between starts of operations
            \item used to emulate input streams with a constant rate
        \end{itemize}
    \item e.g. if OP takes \ns{80} and wait time is \ns{200} then ConstantWait leads to a \ns{280} delay
        between starts of OPs and ConstantRate leads to a \ns{200} delay between starts of OPs because it
        only waits $\ns{200}-\ns{80}=\ns{120}$
    \item Bursty
        \begin{itemize}
            \item defers waits in order to have blocks of BURST\_SIZE elements with no waits
            \item used to emulate enqueue / dequeue behaviour that is bursty
            \item typical for network applications
            \item e.g. with BURST\_SIZE of 20, wait time of \ns{100}, and OP duration of \ns{20} -- 20
                elements will be enqueued followed by a wait of $20\cdot(\ns{100} - \ns{20}) = \ns{1600}$ (thus on average delay between starts of OPs will be \ns{100})
        \end{itemize}
    \item Jitter can be enabled; adds pseudo-random variance to wait times (idea also present in FFWDQ paper)
\end{itemize}

\subsubsection{Measurer Implementations}
\begin{itemize}
    \item only one measurer implemented (=> used for all benchmarks here)
    \item uses \texttt{RDTSC} instruction as explained previously
    \item could vary timestamp source e.g. for different architectures
    \item could also measure failed operations
    \item could measure cache stats for the duration of the operation
\end{itemize}
