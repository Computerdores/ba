\section{Introduction}
\begin{itemize}
    \item many different queue implementations
    \item each paper benchmarks differently
    \item comes to conclusion that their queue is the best
    \item wasn't able to find any repro papers
    \item papers rarely provide source code
    \item even then, source code often incomplete or gone missing
    \item a direct replication not possible (see
            \href{https://plato.stanford.edu/entries/scientific-reproducibility/#:~:text=direct
                replication%20%28which%20closely%20follow%20the%20original%20study%20to%20verify%20results%29%20and
                conceptual%20replications%20%28which%20deliberately%20alter%20important%20features%20of%20the%20study
                to%20generalize%20findings%20or%20to%20test%20the%20underlying%20hypothesis%20in%20a%20new%20way%29
        }{plato.standford.edu})
    \item only conceptual reproduction
    \item I aim to test the performance of six queues: BQ, EQ, FastForwardQ, FastFlowQ, LamportQ, MCRingBuffer
    \item I plan to do so in a manner most similar to how EQ did as they compare the most different
        queues out of the papers of these queues
    \item Lamport not compared in EQ paper but in FFWDQ paper
    \item Further I aim to make my results as replicable as possible, by:
        \begin{itemize}
            \item releasing the full source code, both hosted online and embedded into the pdf
                (latter to prevent missing code in the future)
            \item providing, and explaining my choice of, all parameters of the tests and queues
            \item providing, and explaining the choices made in, all implementations of the queues
        \end{itemize}
    \item I also want to encourage future authors to make their work more replicable and comparable,
        by making my benchmark easy to extend \todo{reword / expand upon}
\end{itemize}
